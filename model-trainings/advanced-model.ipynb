{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["ibFv4_DSVMLh"],"gpuType":"T4","mount_file_id":"10-qwmrCkZweP_ON1qoDrzoA8JdP3nT-R","authorship_tag":"ABX9TyNP6BgSYhW+zbksMHNrcWjo"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["Code to prevent Colab from disconnecting.\n","\n","\n","\n","```\n","function ConnectButton(){\n","  console.log(\"Connect pushed\");\n","  document.querySelector(\"#top-toolbar > colab-connectbutton\").shadowRoot.querySelector(\"#connect\").click()\n","}\n","setInterval(ConnectButton,60000);\n","```\n","\n"],"metadata":{"id":"Hr1qHGFI6osB"}},{"cell_type":"markdown","source":["# install & imports"],"metadata":{"id":"gKRpHbcQUfkr"}},{"cell_type":"code","source":["!pip install ultralytics"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FJ-DwO-9Uf5a","executionInfo":{"status":"ok","timestamp":1705062862352,"user_tz":-180,"elapsed":12150,"user":{"displayName":"Seçilay Kutal (Student)","userId":"13513857455969962860"}},"outputId":"d2ee8a7a-ad59-44e2-a172-0df31de16b9a"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting ultralytics\n","  Downloading ultralytics-8.1.0-py3-none-any.whl (699 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m699.2/699.2 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (3.7.1)\n","Requirement already satisfied: numpy>=1.22.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.23.5)\n","Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.8.0.76)\n","Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.4.0)\n","Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (6.0.1)\n","Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.31.0)\n","Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.11.4)\n","Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.1.0+cu121)\n","Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.16.0+cu121)\n","Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.66.1)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\n","Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.0.0)\n","Collecting thop>=0.1.1 (from ultralytics)\n","  Downloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\n","Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.5.3)\n","Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.12.2)\n","Collecting hub-sdk>=0.0.2 (from ultralytics)\n","  Downloading hub_sdk-0.0.3-py3-none-any.whl (37 kB)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.2.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.47.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (23.2)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.1.1)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2023.3.post1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2023.11.17)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.13.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.1.2)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2023.6.0)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2.1.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\n","Installing collected packages: hub-sdk, thop, ultralytics\n","Successfully installed hub-sdk-0.0.3 thop-0.1.1.post2209072238 ultralytics-8.1.0\n"]}]},{"cell_type":"code","source":["from ultralytics import YOLO\n","import numpy as np\n","import cv2\n","import datetime"],"metadata":{"id":"ovRosHJyWafx","executionInfo":{"status":"ok","timestamp":1705062871584,"user_tz":-180,"elapsed":9236,"user":{"displayName":"Seçilay Kutal (Student)","userId":"13513857455969962860"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["# Training"],"metadata":{"id":"c-V1qP-kU2Ml"}},{"cell_type":"code","source":["PRETRAINED_MODEL = 'yolov8m-cls.pt'\n","DATA_PATH = '/content/drive/MyDrive/courses/fall2023/Internet of Things/Final Project/Image Classification Training YOLOv8/47-class training/dataset'\n","EPOCH = 10"],"metadata":{"id":"-3PpZGQwU7DJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = YOLO(PRETRAINED_MODEL) # load a pretrained model\n","\n","model.train(data=DATA_PATH, epochs=EPOCH) # train the model"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6RxHWFnYU1rn","executionInfo":{"status":"ok","timestamp":1704654546229,"user_tz":-180,"elapsed":1873707,"user":{"displayName":"Seçilay Kutal (Student)","userId":"13513857455969962860"}},"outputId":"695c4e21-dec0-42cb-be85-7d3651fb7380"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Ultralytics YOLOv8.0.237 🚀 Python-3.10.12 torch-2.1.0+cu121 CUDA:0 (Tesla T4, 15102MiB)\n","\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=classify, mode=train, model=yolov8m-cls.pt, data=/content/drive/MyDrive/courses/fall2023/Internet of Things/Final Project/Image Classification Training YOLOv8/47-class training/dataset, epochs=10, time=None, patience=50, batch=16, imgsz=224, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train2, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/classify/train2\n","\u001b[34m\u001b[1mtrain:\u001b[0m /content/drive/MyDrive/courses/fall2023/Internet of Things/Final Project/Image Classification Training YOLOv8/47-class training/dataset/train... found 1907 images in 47 classes ✅ \n","\u001b[34m\u001b[1mval:\u001b[0m None...\n","\u001b[34m\u001b[1mtest:\u001b[0m /content/drive/MyDrive/courses/fall2023/Internet of Things/Final Project/Image Classification Training YOLOv8/47-class training/dataset/test... found 849 images in 47 classes ✅ \n","Overriding model.yaml nc=1000 with nc=47\n","\n","                   from  n    params  module                                       arguments                     \n","  0                  -1  1      1392  ultralytics.nn.modules.conv.Conv             [3, 48, 3, 2]                 \n","  1                  -1  1     41664  ultralytics.nn.modules.conv.Conv             [48, 96, 3, 2]                \n","  2                  -1  2    111360  ultralytics.nn.modules.block.C2f             [96, 96, 2, True]             \n","  3                  -1  1    166272  ultralytics.nn.modules.conv.Conv             [96, 192, 3, 2]               \n","  4                  -1  4    813312  ultralytics.nn.modules.block.C2f             [192, 192, 4, True]           \n","  5                  -1  1    664320  ultralytics.nn.modules.conv.Conv             [192, 384, 3, 2]              \n","  6                  -1  4   3248640  ultralytics.nn.modules.block.C2f             [384, 384, 4, True]           \n","  7                  -1  1   2655744  ultralytics.nn.modules.conv.Conv             [384, 768, 3, 2]              \n","  8                  -1  2   7084032  ultralytics.nn.modules.block.C2f             [768, 768, 2, True]           \n","  9                  -1  1   1045807  ultralytics.nn.modules.head.Classify         [768, 47]                     \n","YOLOv8m-cls summary: 141 layers, 15832543 parameters, 15832543 gradients, 41.9 GFLOPs\n","Transferred 228/230 items from pretrained weights\n","\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/classify/train2', view at http://localhost:6006/\n","\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n","Downloading https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8n.pt to 'yolov8n.pt'...\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 6.23M/6.23M [00:00<00:00, 179MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["WARNING ⚠️ NMS time limit 0.550s exceeded\n","\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/courses/fall2023/Internet of Things/Final Project/Image Classification Training YOLOv8/47-class training/dataset/train... 1907 images, 0 corrupt: 100%|██████████| 1907/1907 [16:33<00:00,  1.92it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/drive/MyDrive/courses/fall2023/Internet of Things/Final Project/Image Classification Training YOLOv8/47-class training/dataset/train.cache\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/courses/fall2023/Internet of Things/Final Project/Image Classification Training YOLOv8/47-class training/dataset/test... 849 images, 0 corrupt: 100%|██████████| 849/849 [07:29<00:00,  1.89it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/drive/MyDrive/courses/fall2023/Internet of Things/Final Project/Image Classification Training YOLOv8/47-class training/dataset/test.cache\n","\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n","\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000714, momentum=0.9) with parameter groups 38 weight(decay=0.0), 39 weight(decay=0.0005), 39 bias(decay=0.0)\n","10 epochs...\n","\n","      Epoch    GPU_mem       loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["       1/10      1.01G      3.954         16        224:   7%|▋         | 8/120 [00:02<00:24,  4.53it/s]"]},{"output_type":"stream","name":"stdout","text":["Downloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf'...\n"]},{"output_type":"stream","name":"stderr","text":["       1/10      1.01G      3.959         16        224:  12%|█▏        | 14/120 [00:03<00:23,  4.56it/s]\n","100%|██████████| 755k/755k [00:00<00:00, 36.6MB/s]\n","       1/10      1.05G      3.631          3        224: 100%|██████████| 120/120 [00:30<00:00,  4.00it/s]\n","               classes   top1_acc   top5_acc: 100%|██████████| 27/27 [00:07<00:00,  3.57it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all      0.396      0.718\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem       loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["       2/10      1.03G      1.944          3        224: 100%|██████████| 120/120 [00:28<00:00,  4.21it/s]\n","               classes   top1_acc   top5_acc: 100%|██████████| 27/27 [00:07<00:00,  3.71it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all      0.721      0.934\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem       loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["       3/10      1.04G      1.214          3        224: 100%|██████████| 120/120 [00:26<00:00,  4.49it/s]\n","               classes   top1_acc   top5_acc: 100%|██████████| 27/27 [00:09<00:00,  2.91it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all       0.77      0.958\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem       loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["       4/10      1.04G     0.8929          3        224: 100%|██████████| 120/120 [00:27<00:00,  4.41it/s]\n","               classes   top1_acc   top5_acc: 100%|██████████| 27/27 [00:07<00:00,  3.42it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all        0.8      0.959\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem       loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["       5/10      1.04G     0.7469          3        224: 100%|██████████| 120/120 [00:28<00:00,  4.21it/s]\n","               classes   top1_acc   top5_acc: 100%|██████████| 27/27 [00:06<00:00,  3.94it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all      0.828      0.972\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem       loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["       6/10      1.03G     0.6093          3        224: 100%|██████████| 120/120 [00:27<00:00,  4.39it/s]\n","               classes   top1_acc   top5_acc: 100%|██████████| 27/27 [00:09<00:00,  2.95it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all      0.821      0.976\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem       loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["       7/10      1.04G     0.4829          3        224: 100%|██████████| 120/120 [00:26<00:00,  4.51it/s]\n","               classes   top1_acc   top5_acc: 100%|██████████| 27/27 [00:09<00:00,  2.98it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all      0.845      0.975\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem       loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["       8/10      1.03G     0.3807          3        224: 100%|██████████| 120/120 [00:28<00:00,  4.20it/s]\n","               classes   top1_acc   top5_acc: 100%|██████████| 27/27 [00:07<00:00,  3.78it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all       0.86       0.98\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem       loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["       9/10      1.03G     0.3119          3        224: 100%|██████████| 120/120 [00:28<00:00,  4.22it/s]\n","               classes   top1_acc   top5_acc: 100%|██████████| 27/27 [00:08<00:00,  3.29it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all       0.86      0.978\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem       loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["      10/10      1.03G     0.2575          3        224: 100%|██████████| 120/120 [00:26<00:00,  4.60it/s]\n","               classes   top1_acc   top5_acc: 100%|██████████| 27/27 [00:09<00:00,  2.95it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all      0.866      0.982\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","10 epochs completed in 0.108 hours.\n","Optimizer stripped from runs/classify/train2/weights/last.pt, 31.8MB\n","Optimizer stripped from runs/classify/train2/weights/best.pt, 31.8MB\n","\n","Validating runs/classify/train2/weights/best.pt...\n","Ultralytics YOLOv8.0.237 🚀 Python-3.10.12 torch-2.1.0+cu121 CUDA:0 (Tesla T4, 15102MiB)\n","YOLOv8m-cls summary (fused): 103 layers, 15822863 parameters, 0 gradients, 41.7 GFLOPs\n","WARNING ⚠️ Dataset 'split=val' not found, using 'split=test' instead.\n","\u001b[34m\u001b[1mtrain:\u001b[0m /content/drive/MyDrive/courses/fall2023/Internet of Things/Final Project/Image Classification Training YOLOv8/47-class training/dataset/train... found 1907 images in 47 classes ✅ \n","\u001b[34m\u001b[1mval:\u001b[0m None...\n","\u001b[34m\u001b[1mtest:\u001b[0m /content/drive/MyDrive/courses/fall2023/Internet of Things/Final Project/Image Classification Training YOLOv8/47-class training/dataset/test... found 849 images in 47 classes ✅ \n"]},{"output_type":"stream","name":"stderr","text":["               classes   top1_acc   top5_acc: 100%|██████████| 27/27 [00:09<00:00,  2.84it/s]\n"]},{"output_type":"stream","name":"stdout","text":["                   all      0.866      0.982\n","Speed: 0.1ms preprocess, 1.1ms inference, 0.0ms loss, 0.0ms postprocess per image\n","Results saved to \u001b[1mruns/classify/train2\u001b[0m\n","Results saved to \u001b[1mruns/classify/train2\u001b[0m\n"]},{"output_type":"execute_result","data":{"text/plain":["ultralytics.utils.metrics.ClassifyMetrics object with attributes:\n","\n","confusion_matrix: <ultralytics.utils.metrics.ConfusionMatrix object at 0x7e9e98494a60>\n","curves: []\n","curves_results: []\n","fitness: 0.9240283071994781\n","keys: ['metrics/accuracy_top1', 'metrics/accuracy_top5']\n","results_dict: {'metrics/accuracy_top1': 0.8657244443893433, 'metrics/accuracy_top5': 0.982332170009613, 'fitness': 0.9240283071994781}\n","save_dir: PosixPath('runs/classify/train2')\n","speed: {'preprocess': 0.09213459927005117, 'inference': 1.147984615905546, 'loss': 0.0007773175818058009, 'postprocess': 0.0005695086907160998}\n","task: 'classify'\n","top1: 0.8657244443893433\n","top5: 0.982332170009613"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["model.names"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C09titb6KQGe","executionInfo":{"status":"ok","timestamp":1704654546230,"user_tz":-180,"elapsed":17,"user":{"displayName":"Seçilay Kutal (Student)","userId":"13513857455969962860"}},"outputId":"45a6cf5d-76c0-4fbc-b2f6-ee157d8e969a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{0: 'Adho Mukha Svanasana',\n"," 1: 'Adho Mukha Vrksasana',\n"," 2: 'Alanasana',\n"," 3: 'Anjaneyasana',\n"," 4: 'Ardha Chandrasana',\n"," 5: 'Ardha Matsyendrasana',\n"," 6: 'Ardha Navasana',\n"," 7: 'Ardha Pincha Mayurasana',\n"," 8: 'Ashta Chandrasana',\n"," 9: 'Baddha Konasana',\n"," 10: 'Bakasana',\n"," 11: 'Balasana',\n"," 12: 'Bitilasana',\n"," 13: 'Camatkarasana',\n"," 14: 'Dhanurasana',\n"," 15: 'Eka Pada Rajakapotasana',\n"," 16: 'Garudasana',\n"," 17: 'Halasana',\n"," 18: 'Hanumanasana',\n"," 19: 'Malasana',\n"," 20: 'Marjaryasana',\n"," 21: 'Navasana',\n"," 22: 'Padmasana',\n"," 23: 'Parsva Virabhadrasana',\n"," 24: 'Parsvottanasana',\n"," 25: 'Paschimottanasana',\n"," 26: 'Phalakasana',\n"," 27: 'Pincha Mayurasana',\n"," 28: 'Salamba Bhujangasana',\n"," 29: 'Salamba Sarvangasana',\n"," 30: 'Setu Bandha Sarvangasana',\n"," 31: 'Sivasana',\n"," 32: 'Supta Kapotasana',\n"," 33: 'Trikonasana',\n"," 34: 'Upavistha Konasana',\n"," 35: 'Urdhva Dhanurasana',\n"," 36: 'Urdhva Mukha Svsnssana',\n"," 37: 'Ustrasana',\n"," 38: 'Utkatasana',\n"," 39: 'Uttanasana',\n"," 40: 'Utthita Hasta Padangusthasana',\n"," 41: 'Utthita Parsvakonasana',\n"," 42: 'Vasisthasana',\n"," 43: 'Virabhadrasana One',\n"," 44: 'Virabhadrasana Three',\n"," 45: 'Virabhadrasana Two',\n"," 46: 'Vrksasana'}"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["!mv runs \"/content/drive/MyDrive/courses/fall2023/Internet of Things/Final Project/Image Classification Training YOLOv8/47-class training/models/Jan07_yolov8m-cls\""],"metadata":{"id":"2A2PbHEkEIY1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Note for Egemen\n","\n","Model skoru top5 accuracy alınca %98 geliyor. Belki direkt nokta atışı hani doğru/yanlış classify etti yapmayı engellemek için örneğin kullanıcının yaptığı pose o anki top5 prediction içinde yer alıyor mu gibi de bakabiliriz. Öneri :)\n","\n","Model skoru son 3 epochtur aynı. O yüzden daha uzun train edelim diyemiyorum. Ama öte yandan ben bu eğitimi default parametrelerle yaptım, parametreleri bir kurcalayabilirim. En basidinden image boyutu bile baya bi etkiler diye düşünüyorum.\n","\n","Bir de benim kullandığım pretrained modelin bir büyüğü daha var. Onu da deneyelim derim. Nasılsa training için vaktimiz var. Ama bir yandan appi hazırlayıp sonra model improvement aşamasına dönsek daha iyi olacak gibi bence."],"metadata":{"id":"kScyRYpmV10A"}},{"cell_type":"markdown","source":["# Inference"],"metadata":{"id":"ibFv4_DSVMLh"}},{"cell_type":"code","source":["VIDEO_PATH = \"/content/drive/MyDrive/courses/fall2023/Internet of Things/Final Project/Image Classification Training YOLOv8/VideoToTest/video_1_trimmed.mp4\"\n","BEST_MODEL = \"/content/drive/MyDrive/courses/fall2023/Internet of Things/Final Project/Image Classification Training YOLOv8/47-class training/models/Jan07_yolov8m-cls/classify/train2/weights/best.pt\""],"metadata":{"id":"o3Ny2jr-ZAcC","executionInfo":{"status":"ok","timestamp":1705062872167,"user_tz":-180,"elapsed":3,"user":{"displayName":"Seçilay Kutal (Student)","userId":"13513857455969962860"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["def get_predictions(video=0, DEBUG=True, breakWhen=10):\n","    \"\"\"\n","    Get predictions from the model\n","    model: path to the YOLO model\n","    video: path to the video OR 0 for webcam\n","    threshold: confidence threshold\n","    \"\"\"\n","\n","    video = cv2.VideoCapture(video) # get the video\n","    model = YOLO(BEST_MODEL) # get the model\n","\n","    CLASSES = model.names\n","\n","\n","    predictions = []\n","\n","\n","    while video.isOpened():\n","\n","        _, frame = video.read()\n","        results = model(frame, verbose=False)\n","\n","        for r in results:\n","\n","            cls = CLASSES[r.probs.top1]\n","            score = r.probs.top1conf.cpu()\n","\n","            if not predictions or predictions[-1][\"class\"] != cls:\n","\n","                if predictions:\n","                    predictions[-1][\"final_timestamp\"] = video.get(cv2.CAP_PROP_POS_MSEC)\n","                    predictions[-1][\"duration\"] = datetime.timedelta(milliseconds=predictions[-1][\"final_timestamp\"] - predictions[-1][\"start_timestamp\"])\n","                    predictions[-1][\"avg_score\"] = np.average(predictions[-1][\"scores\"])\n","\n","                    if DEBUG:\n","                        print(f\"Detected pose:\", predictions[-1][\"class\"], \"\\tDuration:\", predictions[-1][\"duration\"], \"\\tScore:\", predictions[-1][\"avg_score\"])\n","\n","                predictions.append(\n","                    {\n","                        \"class\": cls,\n","                        \"scores\": [score],\n","                        \"avg_score\": None,\n","                        \"final_timestamp\": None,\n","                        \"start_timestamp\": video.get(cv2.CAP_PROP_POS_MSEC),\n","                        \"duration\": None\n","                    }\n","                )\n","\n","            else:\n","                predictions[-1][\"scores\"].append(score)\n","\n","            # if DEBUG:\n","            #     print(\"*\"*20)\n","            #     print(f\"Pose: {cls}, Score: {score:.2f}\")\n","        if len(predictions) > breakWhen:\n","            break\n","\n","    video.release()\n","    cv2.destroyAllWindows()\n","\n","    return predictions"],"metadata":{"id":"rNebJq47MIYM","executionInfo":{"status":"ok","timestamp":1705062944109,"user_tz":-180,"elapsed":303,"user":{"displayName":"Seçilay Kutal (Student)","userId":"13513857455969962860"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["predictions = get_predictions(video=VIDEO_PATH, DEBUG=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"omiN0jUNMnWR","executionInfo":{"status":"ok","timestamp":1705062952823,"user_tz":-180,"elapsed":8398,"user":{"displayName":"Seçilay Kutal (Student)","userId":"13513857455969962860"}},"outputId":"9c591fa0-45af-4db6-e605-2b7f33bd854d"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Detected pose: Marjaryasana \tDuration: 0:00:00.116750 \tScore: 0.74751097\n","Detected pose: Bitilasana \tDuration: 0:00:05.238572 \tScore: 0.92878926\n","Detected pose: Marjaryasana \tDuration: 0:00:04.137471 \tScore: 0.9043481\n","Detected pose: Bitilasana \tDuration: 0:00:00.433767 \tScore: 0.61137986\n","Detected pose: Marjaryasana \tDuration: 0:00:00.033367 \tScore: 0.51161593\n","Detected pose: Bitilasana \tDuration: 0:00:00.033367 \tScore: 0.5002962\n","Detected pose: Marjaryasana \tDuration: 0:00:00.166834 \tScore: 0.5160438\n","Detected pose: Bitilasana \tDuration: 0:00:01.001001 \tScore: 0.58258\n","Detected pose: Marjaryasana \tDuration: 0:00:00.033367 \tScore: 0.51860994\n","Detected pose: Bitilasana \tDuration: 0:00:01.468135 \tScore: 0.7204194\n"]}]},{"cell_type":"code","source":["for pred in predictions[:-1]: # since we interrupted the above code, the last value is None\n","    print(\"Pose: \", pred['class'] + \" \"*(18-pred['class'].__len__()), f\"\\t\\t Score: {pred['avg_score']:.2f} \\t\\t Duration: {pred['duration']}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PezVoKZcoy-l","executionInfo":{"status":"ok","timestamp":1704654718029,"user_tz":-180,"elapsed":395,"user":{"displayName":"Seçilay Kutal (Student)","userId":"13513857455969962860"}},"outputId":"92a4c357-308c-427b-f1bb-97cb8143a652"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Pose:  Padmasana          \t\t Score: 0.71 \t\t Duration: 0:00:02.469133\n","Pose:  Baddha Konasana    \t\t Score: 0.75 \t\t Duration: 0:00:05.905900\n","Pose:  Upavistha Konasana \t\t Score: 0.44 \t\t Duration: 0:00:00.100100\n","Pose:  Baddha Konasana    \t\t Score: 0.70 \t\t Duration: 0:00:06.039367\n","Pose:  Upavistha Konasana \t\t Score: 0.40 \t\t Duration: 0:00:00.133467\n","Pose:  Baddha Konasana    \t\t Score: 0.39 \t\t Duration: 0:00:00.066733\n","Pose:  Upavistha Konasana \t\t Score: 0.40 \t\t Duration: 0:00:00.066733\n","Pose:  Baddha Konasana    \t\t Score: 0.65 \t\t Duration: 0:00:09.776433\n","Pose:  Padmasana          \t\t Score: 0.52 \t\t Duration: 0:00:03.703700\n","Pose:  Baddha Konasana    \t\t Score: 0.77 \t\t Duration: 0:00:03.436767\n"]}]}]}