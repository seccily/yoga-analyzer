{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["ibFv4_DSVMLh"],"gpuType":"T4","mount_file_id":"10-qwmrCkZweP_ON1qoDrzoA8JdP3nT-R","authorship_tag":"ABX9TyNP6BgSYhW+zbksMHNrcWjo"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["Code to prevent Colab from disconnecting.\n","\n","\n","\n","```\n","function ConnectButton(){\n","  console.log(\"Connect pushed\");\n","  document.querySelector(\"#top-toolbar > colab-connectbutton\").shadowRoot.querySelector(\"#connect\").click()\n","}\n","setInterval(ConnectButton,60000);\n","```\n","\n"],"metadata":{"id":"Hr1qHGFI6osB"}},{"cell_type":"markdown","source":["# install & imports"],"metadata":{"id":"gKRpHbcQUfkr"}},{"cell_type":"code","source":["!pip install ultralytics"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FJ-DwO-9Uf5a","executionInfo":{"status":"ok","timestamp":1705062862352,"user_tz":-180,"elapsed":12150,"user":{"displayName":"SeÃ§ilay Kutal (Student)","userId":"13513857455969962860"}},"outputId":"d2ee8a7a-ad59-44e2-a172-0df31de16b9a"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting ultralytics\n","  Downloading ultralytics-8.1.0-py3-none-any.whl (699 kB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m699.2/699.2 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (3.7.1)\n","Requirement already satisfied: numpy>=1.22.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.23.5)\n","Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.8.0.76)\n","Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.4.0)\n","Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (6.0.1)\n","Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.31.0)\n","Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.11.4)\n","Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.1.0+cu121)\n","Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.16.0+cu121)\n","Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.66.1)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\n","Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.0.0)\n","Collecting thop>=0.1.1 (from ultralytics)\n","  Downloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\n","Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.5.3)\n","Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.12.2)\n","Collecting hub-sdk>=0.0.2 (from ultralytics)\n","  Downloading hub_sdk-0.0.3-py3-none-any.whl (37 kB)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.2.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.47.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (23.2)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.1.1)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2023.3.post1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2023.11.17)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.13.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.1.2)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2023.6.0)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2.1.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\n","Installing collected packages: hub-sdk, thop, ultralytics\n","Successfully installed hub-sdk-0.0.3 thop-0.1.1.post2209072238 ultralytics-8.1.0\n"]}]},{"cell_type":"code","source":["from ultralytics import YOLO\n","import numpy as np\n","import cv2\n","import datetime"],"metadata":{"id":"ovRosHJyWafx","executionInfo":{"status":"ok","timestamp":1705062871584,"user_tz":-180,"elapsed":9236,"user":{"displayName":"SeÃ§ilay Kutal (Student)","userId":"13513857455969962860"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["# Training"],"metadata":{"id":"c-V1qP-kU2Ml"}},{"cell_type":"code","source":["PRETRAINED_MODEL = 'yolov8m-cls.pt'\n","DATA_PATH = '/content/drive/MyDrive/courses/fall2023/Internet of Things/Final Project/Image Classification Training YOLOv8/47-class training/dataset'\n","EPOCH = 10"],"metadata":{"id":"-3PpZGQwU7DJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = YOLO(PRETRAINED_MODEL) # load a pretrained model\n","\n","model.train(data=DATA_PATH, epochs=EPOCH) # train the model"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6RxHWFnYU1rn","executionInfo":{"status":"ok","timestamp":1704654546229,"user_tz":-180,"elapsed":1873707,"user":{"displayName":"SeÃ§ilay Kutal (Student)","userId":"13513857455969962860"}},"outputId":"695c4e21-dec0-42cb-be85-7d3651fb7380"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Ultralytics YOLOv8.0.237 ðŸš€ Python-3.10.12 torch-2.1.0+cu121 CUDA:0 (Tesla T4, 15102MiB)\n","\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=classify, mode=train, model=yolov8m-cls.pt, data=/content/drive/MyDrive/courses/fall2023/Internet of Things/Final Project/Image Classification Training YOLOv8/47-class training/dataset, epochs=10, time=None, patience=50, batch=16, imgsz=224, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train2, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/classify/train2\n","\u001b[34m\u001b[1mtrain:\u001b[0m /content/drive/MyDrive/courses/fall2023/Internet of Things/Final Project/Image Classification Training YOLOv8/47-class training/dataset/train... found 1907 images in 47 classes âœ… \n","\u001b[34m\u001b[1mval:\u001b[0m None...\n","\u001b[34m\u001b[1mtest:\u001b[0m /content/drive/MyDrive/courses/fall2023/Internet of Things/Final Project/Image Classification Training YOLOv8/47-class training/dataset/test... found 849 images in 47 classes âœ… \n","Overriding model.yaml nc=1000 with nc=47\n","\n","                   from  n    params  module                                       arguments                     \n","  0                  -1  1      1392  ultralytics.nn.modules.conv.Conv             [3, 48, 3, 2]                 \n","  1                  -1  1     41664  ultralytics.nn.modules.conv.Conv             [48, 96, 3, 2]                \n","  2                  -1  2    111360  ultralytics.nn.modules.block.C2f             [96, 96, 2, True]             \n","  3                  -1  1    166272  ultralytics.nn.modules.conv.Conv             [96, 192, 3, 2]               \n","  4                  -1  4    813312  ultralytics.nn.modules.block.C2f             [192, 192, 4, True]           \n","  5                  -1  1    664320  ultralytics.nn.modules.conv.Conv             [192, 384, 3, 2]              \n","  6                  -1  4   3248640  ultralytics.nn.modules.block.C2f             [384, 384, 4, True]           \n","  7                  -1  1   2655744  ultralytics.nn.modules.conv.Conv             [384, 768, 3, 2]              \n","  8                  -1  2   7084032  ultralytics.nn.modules.block.C2f             [768, 768, 2, True]           \n","  9                  -1  1   1045807  ultralytics.nn.modules.head.Classify         [768, 47]                     \n","YOLOv8m-cls summary: 141 layers, 15832543 parameters, 15832543 gradients, 41.9 GFLOPs\n","Transferred 228/230 items from pretrained weights\n","\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/classify/train2', view at http://localhost:6006/\n","\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n","Downloading https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8n.pt to 'yolov8n.pt'...\n"]},{"output_type":"stream","name":"stderr","text":["100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6.23M/6.23M [00:00<00:00, 179MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["WARNING âš ï¸ NMS time limit 0.550s exceeded\n","\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/courses/fall2023/Internet of Things/Final Project/Image Classification Training YOLOv8/47-class training/dataset/train... 1907 images, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1907/1907 [16:33<00:00,  1.92it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/drive/MyDrive/courses/fall2023/Internet of Things/Final Project/Image Classification Training YOLOv8/47-class training/dataset/train.cache\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/courses/fall2023/Internet of Things/Final Project/Image Classification Training YOLOv8/47-class training/dataset/test... 849 images, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 849/849 [07:29<00:00,  1.89it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/drive/MyDrive/courses/fall2023/Internet of Things/Final Project/Image Classification Training YOLOv8/47-class training/dataset/test.cache\n","\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n","\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000714, momentum=0.9) with parameter groups 38 weight(decay=0.0), 39 weight(decay=0.0005), 39 bias(decay=0.0)\n","10 epochs...\n","\n","      Epoch    GPU_mem       loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["       1/10      1.01G      3.954         16        224:   7%|â–‹         | 8/120 [00:02<00:24,  4.53it/s]"]},{"output_type":"stream","name":"stdout","text":["Downloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf'...\n"]},{"output_type":"stream","name":"stderr","text":["       1/10      1.01G      3.959         16        224:  12%|â–ˆâ–        | 14/120 [00:03<00:23,  4.56it/s]\n","100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 755k/755k [00:00<00:00, 36.6MB/s]\n","       1/10      1.05G      3.631          3        224: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 120/120 [00:30<00:00,  4.00it/s]\n","               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:07<00:00,  3.57it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all      0.396      0.718\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem       loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["       2/10      1.03G      1.944          3        224: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 120/120 [00:28<00:00,  4.21it/s]\n","               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:07<00:00,  3.71it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all      0.721      0.934\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem       loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["       3/10      1.04G      1.214          3        224: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 120/120 [00:26<00:00,  4.49it/s]\n","               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:09<00:00,  2.91it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all       0.77      0.958\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem       loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["       4/10      1.04G     0.8929          3        224: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 120/120 [00:27<00:00,  4.41it/s]\n","               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:07<00:00,  3.42it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all        0.8      0.959\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem       loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["       5/10      1.04G     0.7469          3        224: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 120/120 [00:28<00:00,  4.21it/s]\n","               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:06<00:00,  3.94it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all      0.828      0.972\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem       loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["       6/10      1.03G     0.6093          3        224: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 120/120 [00:27<00:00,  4.39it/s]\n","               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:09<00:00,  2.95it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all      0.821      0.976\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem       loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["       7/10      1.04G     0.4829          3        224: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 120/120 [00:26<00:00,  4.51it/s]\n","               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:09<00:00,  2.98it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all      0.845      0.975\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem       loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["       8/10      1.03G     0.3807          3        224: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 120/120 [00:28<00:00,  4.20it/s]\n","               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:07<00:00,  3.78it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all       0.86       0.98\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem       loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["       9/10      1.03G     0.3119          3        224: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 120/120 [00:28<00:00,  4.22it/s]\n","               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:08<00:00,  3.29it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all       0.86      0.978\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem       loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["      10/10      1.03G     0.2575          3        224: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 120/120 [00:26<00:00,  4.60it/s]\n","               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:09<00:00,  2.95it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all      0.866      0.982\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","10 epochs completed in 0.108 hours.\n","Optimizer stripped from runs/classify/train2/weights/last.pt, 31.8MB\n","Optimizer stripped from runs/classify/train2/weights/best.pt, 31.8MB\n","\n","Validating runs/classify/train2/weights/best.pt...\n","Ultralytics YOLOv8.0.237 ðŸš€ Python-3.10.12 torch-2.1.0+cu121 CUDA:0 (Tesla T4, 15102MiB)\n","YOLOv8m-cls summary (fused): 103 layers, 15822863 parameters, 0 gradients, 41.7 GFLOPs\n","WARNING âš ï¸ Dataset 'split=val' not found, using 'split=test' instead.\n","\u001b[34m\u001b[1mtrain:\u001b[0m /content/drive/MyDrive/courses/fall2023/Internet of Things/Final Project/Image Classification Training YOLOv8/47-class training/dataset/train... found 1907 images in 47 classes âœ… \n","\u001b[34m\u001b[1mval:\u001b[0m None...\n","\u001b[34m\u001b[1mtest:\u001b[0m /content/drive/MyDrive/courses/fall2023/Internet of Things/Final Project/Image Classification Training YOLOv8/47-class training/dataset/test... found 849 images in 47 classes âœ… \n"]},{"output_type":"stream","name":"stderr","text":["               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:09<00:00,  2.84it/s]\n"]},{"output_type":"stream","name":"stdout","text":["                   all      0.866      0.982\n","Speed: 0.1ms preprocess, 1.1ms inference, 0.0ms loss, 0.0ms postprocess per image\n","Results saved to \u001b[1mruns/classify/train2\u001b[0m\n","Results saved to \u001b[1mruns/classify/train2\u001b[0m\n"]},{"output_type":"execute_result","data":{"text/plain":["ultralytics.utils.metrics.ClassifyMetrics object with attributes:\n","\n","confusion_matrix: <ultralytics.utils.metrics.ConfusionMatrix object at 0x7e9e98494a60>\n","curves: []\n","curves_results: []\n","fitness: 0.9240283071994781\n","keys: ['metrics/accuracy_top1', 'metrics/accuracy_top5']\n","results_dict: {'metrics/accuracy_top1': 0.8657244443893433, 'metrics/accuracy_top5': 0.982332170009613, 'fitness': 0.9240283071994781}\n","save_dir: PosixPath('runs/classify/train2')\n","speed: {'preprocess': 0.09213459927005117, 'inference': 1.147984615905546, 'loss': 0.0007773175818058009, 'postprocess': 0.0005695086907160998}\n","task: 'classify'\n","top1: 0.8657244443893433\n","top5: 0.982332170009613"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["model.names"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C09titb6KQGe","executionInfo":{"status":"ok","timestamp":1704654546230,"user_tz":-180,"elapsed":17,"user":{"displayName":"SeÃ§ilay Kutal (Student)","userId":"13513857455969962860"}},"outputId":"45a6cf5d-76c0-4fbc-b2f6-ee157d8e969a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{0: 'Adho Mukha Svanasana',\n"," 1: 'Adho Mukha Vrksasana',\n"," 2: 'Alanasana',\n"," 3: 'Anjaneyasana',\n"," 4: 'Ardha Chandrasana',\n"," 5: 'Ardha Matsyendrasana',\n"," 6: 'Ardha Navasana',\n"," 7: 'Ardha Pincha Mayurasana',\n"," 8: 'Ashta Chandrasana',\n"," 9: 'Baddha Konasana',\n"," 10: 'Bakasana',\n"," 11: 'Balasana',\n"," 12: 'Bitilasana',\n"," 13: 'Camatkarasana',\n"," 14: 'Dhanurasana',\n"," 15: 'Eka Pada Rajakapotasana',\n"," 16: 'Garudasana',\n"," 17: 'Halasana',\n"," 18: 'Hanumanasana',\n"," 19: 'Malasana',\n"," 20: 'Marjaryasana',\n"," 21: 'Navasana',\n"," 22: 'Padmasana',\n"," 23: 'Parsva Virabhadrasana',\n"," 24: 'Parsvottanasana',\n"," 25: 'Paschimottanasana',\n"," 26: 'Phalakasana',\n"," 27: 'Pincha Mayurasana',\n"," 28: 'Salamba Bhujangasana',\n"," 29: 'Salamba Sarvangasana',\n"," 30: 'Setu Bandha Sarvangasana',\n"," 31: 'Sivasana',\n"," 32: 'Supta Kapotasana',\n"," 33: 'Trikonasana',\n"," 34: 'Upavistha Konasana',\n"," 35: 'Urdhva Dhanurasana',\n"," 36: 'Urdhva Mukha Svsnssana',\n"," 37: 'Ustrasana',\n"," 38: 'Utkatasana',\n"," 39: 'Uttanasana',\n"," 40: 'Utthita Hasta Padangusthasana',\n"," 41: 'Utthita Parsvakonasana',\n"," 42: 'Vasisthasana',\n"," 43: 'Virabhadrasana One',\n"," 44: 'Virabhadrasana Three',\n"," 45: 'Virabhadrasana Two',\n"," 46: 'Vrksasana'}"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["!mv runs \"/content/drive/MyDrive/courses/fall2023/Internet of Things/Final Project/Image Classification Training YOLOv8/47-class training/models/Jan07_yolov8m-cls\""],"metadata":{"id":"2A2PbHEkEIY1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Note for Egemen\n","\n","Model skoru top5 accuracy alÄ±nca %98 geliyor. Belki direkt nokta atÄ±ÅŸÄ± hani doÄŸru/yanlÄ±ÅŸ classify etti yapmayÄ± engellemek iÃ§in Ã¶rneÄŸin kullanÄ±cÄ±nÄ±n yaptÄ±ÄŸÄ± pose o anki top5 prediction iÃ§inde yer alÄ±yor mu gibi de bakabiliriz. Ã–neri :)\n","\n","Model skoru son 3 epochtur aynÄ±. O yÃ¼zden daha uzun train edelim diyemiyorum. Ama Ã¶te yandan ben bu eÄŸitimi default parametrelerle yaptÄ±m, parametreleri bir kurcalayabilirim. En basidinden image boyutu bile baya bi etkiler diye dÃ¼ÅŸÃ¼nÃ¼yorum.\n","\n","Bir de benim kullandÄ±ÄŸÄ±m pretrained modelin bir bÃ¼yÃ¼ÄŸÃ¼ daha var. Onu da deneyelim derim. NasÄ±lsa training iÃ§in vaktimiz var. Ama bir yandan appi hazÄ±rlayÄ±p sonra model improvement aÅŸamasÄ±na dÃ¶nsek daha iyi olacak gibi bence."],"metadata":{"id":"kScyRYpmV10A"}},{"cell_type":"markdown","source":["# Inference"],"metadata":{"id":"ibFv4_DSVMLh"}},{"cell_type":"code","source":["VIDEO_PATH = \"/content/drive/MyDrive/courses/fall2023/Internet of Things/Final Project/Image Classification Training YOLOv8/VideoToTest/video_1_trimmed.mp4\"\n","BEST_MODEL = \"/content/drive/MyDrive/courses/fall2023/Internet of Things/Final Project/Image Classification Training YOLOv8/47-class training/models/Jan07_yolov8m-cls/classify/train2/weights/best.pt\""],"metadata":{"id":"o3Ny2jr-ZAcC","executionInfo":{"status":"ok","timestamp":1705062872167,"user_tz":-180,"elapsed":3,"user":{"displayName":"SeÃ§ilay Kutal (Student)","userId":"13513857455969962860"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["def get_predictions(video=0, DEBUG=True, breakWhen=10):\n","    \"\"\"\n","    Get predictions from the model\n","    model: path to the YOLO model\n","    video: path to the video OR 0 for webcam\n","    threshold: confidence threshold\n","    \"\"\"\n","\n","    video = cv2.VideoCapture(video) # get the video\n","    model = YOLO(BEST_MODEL) # get the model\n","\n","    CLASSES = model.names\n","\n","\n","    predictions = []\n","\n","\n","    while video.isOpened():\n","\n","        _, frame = video.read()\n","        results = model(frame, verbose=False)\n","\n","        for r in results:\n","\n","            cls = CLASSES[r.probs.top1]\n","            score = r.probs.top1conf.cpu()\n","\n","            if not predictions or predictions[-1][\"class\"] != cls:\n","\n","                if predictions:\n","                    predictions[-1][\"final_timestamp\"] = video.get(cv2.CAP_PROP_POS_MSEC)\n","                    predictions[-1][\"duration\"] = datetime.timedelta(milliseconds=predictions[-1][\"final_timestamp\"] - predictions[-1][\"start_timestamp\"])\n","                    predictions[-1][\"avg_score\"] = np.average(predictions[-1][\"scores\"])\n","\n","                    if DEBUG:\n","                        print(f\"Detected pose:\", predictions[-1][\"class\"], \"\\tDuration:\", predictions[-1][\"duration\"], \"\\tScore:\", predictions[-1][\"avg_score\"])\n","\n","                predictions.append(\n","                    {\n","                        \"class\": cls,\n","                        \"scores\": [score],\n","                        \"avg_score\": None,\n","                        \"final_timestamp\": None,\n","                        \"start_timestamp\": video.get(cv2.CAP_PROP_POS_MSEC),\n","                        \"duration\": None\n","                    }\n","                )\n","\n","            else:\n","                predictions[-1][\"scores\"].append(score)\n","\n","            # if DEBUG:\n","            #     print(\"*\"*20)\n","            #     print(f\"Pose: {cls}, Score: {score:.2f}\")\n","        if len(predictions) > breakWhen:\n","            break\n","\n","    video.release()\n","    cv2.destroyAllWindows()\n","\n","    return predictions"],"metadata":{"id":"rNebJq47MIYM","executionInfo":{"status":"ok","timestamp":1705062944109,"user_tz":-180,"elapsed":303,"user":{"displayName":"SeÃ§ilay Kutal (Student)","userId":"13513857455969962860"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["predictions = get_predictions(video=VIDEO_PATH, DEBUG=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"omiN0jUNMnWR","executionInfo":{"status":"ok","timestamp":1705062952823,"user_tz":-180,"elapsed":8398,"user":{"displayName":"SeÃ§ilay Kutal (Student)","userId":"13513857455969962860"}},"outputId":"9c591fa0-45af-4db6-e605-2b7f33bd854d"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Detected pose: Marjaryasana \tDuration: 0:00:00.116750 \tScore: 0.74751097\n","Detected pose: Bitilasana \tDuration: 0:00:05.238572 \tScore: 0.92878926\n","Detected pose: Marjaryasana \tDuration: 0:00:04.137471 \tScore: 0.9043481\n","Detected pose: Bitilasana \tDuration: 0:00:00.433767 \tScore: 0.61137986\n","Detected pose: Marjaryasana \tDuration: 0:00:00.033367 \tScore: 0.51161593\n","Detected pose: Bitilasana \tDuration: 0:00:00.033367 \tScore: 0.5002962\n","Detected pose: Marjaryasana \tDuration: 0:00:00.166834 \tScore: 0.5160438\n","Detected pose: Bitilasana \tDuration: 0:00:01.001001 \tScore: 0.58258\n","Detected pose: Marjaryasana \tDuration: 0:00:00.033367 \tScore: 0.51860994\n","Detected pose: Bitilasana \tDuration: 0:00:01.468135 \tScore: 0.7204194\n"]}]},{"cell_type":"code","source":["for pred in predictions[:-1]: # since we interrupted the above code, the last value is None\n","    print(\"Pose: \", pred['class'] + \" \"*(18-pred['class'].__len__()), f\"\\t\\t Score: {pred['avg_score']:.2f} \\t\\t Duration: {pred['duration']}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PezVoKZcoy-l","executionInfo":{"status":"ok","timestamp":1704654718029,"user_tz":-180,"elapsed":395,"user":{"displayName":"SeÃ§ilay Kutal (Student)","userId":"13513857455969962860"}},"outputId":"92a4c357-308c-427b-f1bb-97cb8143a652"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Pose:  Padmasana          \t\t Score: 0.71 \t\t Duration: 0:00:02.469133\n","Pose:  Baddha Konasana    \t\t Score: 0.75 \t\t Duration: 0:00:05.905900\n","Pose:  Upavistha Konasana \t\t Score: 0.44 \t\t Duration: 0:00:00.100100\n","Pose:  Baddha Konasana    \t\t Score: 0.70 \t\t Duration: 0:00:06.039367\n","Pose:  Upavistha Konasana \t\t Score: 0.40 \t\t Duration: 0:00:00.133467\n","Pose:  Baddha Konasana    \t\t Score: 0.39 \t\t Duration: 0:00:00.066733\n","Pose:  Upavistha Konasana \t\t Score: 0.40 \t\t Duration: 0:00:00.066733\n","Pose:  Baddha Konasana    \t\t Score: 0.65 \t\t Duration: 0:00:09.776433\n","Pose:  Padmasana          \t\t Score: 0.52 \t\t Duration: 0:00:03.703700\n","Pose:  Baddha Konasana    \t\t Score: 0.77 \t\t Duration: 0:00:03.436767\n"]}]}]}